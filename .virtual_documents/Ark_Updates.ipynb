


import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime, timedelta
import numpy as np

# Load the update data
ark_update_data = pd.read_csv('data/ark/ark_updates_collection.csv')

# Display the first few rows of the data
print(ark_update_data.head())


# Convert the 'Date' column to datetime
ark_update_data['Date'] = pd.to_datetime(ark_update_data['Date'], format='%Y-%m-%d')

# Extract major and minor version numbers
ark_update_data['Major_Version'] = ark_update_data['Version'].str.extract('v(\d+)').astype(float)
ark_update_data['Minor_Version'] = ark_update_data['Version'].str.extract('v\d+\.(\d+)').astype(float)

# Fill NaN values in Minor_Version with 0
ark_update_data['Minor_Version'] = ark_update_data['Minor_Version'].fillna(0)

# Create a combined version number
ark_update_data['Combined_Version'] = ark_update_data['Major_Version'] + ark_update_data['Minor_Version'] / 100

# Sort the dataframe by date
ark_update_data = ark_update_data.sort_values('Date')

# Reset the index
ark_update_data = ark_update_data.reset_index(drop=True)

print(ark_update_data.head())


# Group by major version and aggregate data
ark_grouped_df = ark_update_data.groupby('Major_Version').agg({
    'Date': ['min', 'max'],
    'Minor_Version': 'count',
    'Num_Changes': 'sum'
}).reset_index()

# Rename columns
ark_grouped_df.columns = ['Major_Version', 'Start_Date', 'End_Date', 'Minor_Versions', 'Total_Changes']

print(ark_grouped_df)


# Identify the anomalous versions
anomalous_versions = ark_grouped_df[ark_grouped_df['Major_Version'] > 400]

# Get the date range for the anomalous versions
anomalous_start = anomalous_versions['Start_Date'].min()
anomalous_end = anomalous_versions['End_Date'].max()

# Print the change categories for the anomalous versions
print("Change categories for anomalous versions:")
anomalous_changes = ark_update_data[
    (ark_update_data['Major_Version'] > 400) & 
    (ark_update_data['Date'] >= anomalous_start) & 
    (ark_update_data['Date'] <= anomalous_end)
]
print(anomalous_changes[['Date', 'Version', 'Category', 'Changes']])

# Check for overlapping updates
overlapping_updates = ark_update_data[
    (ark_update_data['Major_Version'] <= 400) & 
    (ark_update_data['Date'] >= anomalous_start) & 
    (ark_update_data['Date'] <= anomalous_end)
]

if not overlapping_updates.empty:
    print("\nOverlapping updates during the same period:")
    print(overlapping_updates[['Date', 'Version', 'Category', 'Changes']])
else:
    print("\nNo overlapping updates found during this period.")

# Print the updates just before and after the anomalous period
buffer_days = 7  # Adjust this to look further before/after if needed
print(f"\nUpdates within {buffer_days} days before the anomalous period:")
before_anomaly = ark_update_data[
    (ark_update_data['Date'] >= anomalous_start - pd.Timedelta(days=buffer_days)) & 
    (ark_update_data['Date'] < anomalous_start)
]
print(before_anomaly[['Date', 'Version', 'Category', 'Changes']])

print(f"\nUpdates within {buffer_days} days after the anomalous period:")
after_anomaly = ark_update_data[
    (ark_update_data['Date'] > anomalous_end) & 
    (ark_update_data['Date'] <= anomalous_end + pd.Timedelta(days=buffer_days))
]
print(after_anomaly[['Date', 'Version', 'Category', 'Changes']])


# Creates a figure and a set of subplots
fig, ax = plt.subplots(figsize=(15, 8))

# Creates the scatter plot
scatter = sns.scatterplot(data=ark_grouped_df, 
                          x='Start_Date', 
                          y='Major_Version', 
                          hue='Minor_Versions', 
                          palette='viridis',
                          s=100,
                          ax=ax)

# Customizing the plot
ax.set_title('Ark Updates Over Time (Updates Ended)', fontsize=16)
ax.set_xlabel('Date', fontsize=12)
ax.set_ylabel('Major Version', fontsize=12)
plt.xticks(rotation=45)

# Adds colorbar legend
norm = plt.Normalize(ark_grouped_df['Minor_Versions'].min(), ark_grouped_df['Minor_Versions'].max())
sm = plt.cm.ScalarMappable(cmap="viridis", norm=norm)
sm.set_array([])
cbar = fig.colorbar(sm, ax=ax, label="Number of Minor Versions")
cbar.ax.tick_params(labelsize=10)

# Removes the automatic legend
ax.get_legend().remove()

# Sets x-axis limits
start_date = datetime(2019, 1, 1)
end_date = datetime(2024, 6, 30)
ax.set_xlim(start_date, end_date)

# Find the last update date
last_update_date = ark_grouped_df['Start_Date'].max()

# Add a vertical line to indicate the end of updates
ax.axvline(x=last_update_date, color='r', linestyle='--', label='End of Updates')

# Add text annotation for end of updates
ax.text(last_update_date + timedelta(days=30), ax.get_ylim()[1], 'Updates Ended', 
        rotation=90, verticalalignment='top', color='r')

# Shade the area after updates ended
ax.axvspan(last_update_date, end_date, alpha=0.2, color='gray')

# Add a trend line (only up to the last update)
dates_ordinal = [date.toordinal() for date in ark_grouped_df['Start_Date']]
major_versions = ark_grouped_df['Major_Version'].values

# Use numpy's polyfit to get the trend line coefficients
z = np.polyfit(dates_ordinal, major_versions, 1)
p = np.poly1d(z)

# Generate points for the trend line
trend_dates = pd.date_range(start=ark_grouped_df['Start_Date'].min(), end=last_update_date)
trend_dates_ordinal = [date.toordinal() for date in trend_dates]
trend_values = p(trend_dates_ordinal)

# Plot the trend line
ax.plot(trend_dates, trend_values, "g--", alpha=0.8, label='Update Trend')

# Add legend
ax.legend()

# Adjust layout and display
plt.tight_layout()
plt.show()

# Print summary statistics and update frequency
print(ark_grouped_df.describe())
update_frequency = ark_grouped_df['Start_Date'].diff().mean()
print(f"Average time between major updates: {update_frequency.days} days")
print(f"Last update was on: {last_update_date.strftime('%Y-%m-%d')}")


# Monthly resampling and feature engineering
ark_monthly = ark_grouped_df.set_index('Start_Date').resample('ME').agg({
    'Major_Version': 'last',
    'Minor_Versions': 'sum',
    'Total_Changes': 'sum'
}).reset_index()

# Fill NaN values with forward fill method
ark_monthly = ark_monthly.ffill()

# Create feature for cumulative changes
ark_monthly['Cumulative_Changes'] = ark_monthly['Total_Changes'].cumsum()

# Create a feature for days since last update
ark_monthly['Days_Since_Last_Update'] = ark_monthly['Start_Date'].diff().dt.days

# Calculate update frequency (number of updates per day)
ark_monthly['Days_In_Period'] = ark_monthly['Start_Date'].diff().dt.days.fillna(30)  # Fill the first period with 30 days
ark_monthly['Update_Frequency'] = ark_monthly['Total_Changes'] / ark_monthly['Days_In_Period']

# Visualize the monthly data
plt.figure(figsize=(15, 25))

plt.subplot(5, 1, 1)
plt.plot(ark_monthly['Start_Date'], ark_monthly['Total_Changes'])
plt.title('Monthly Total Changes')
plt.ylabel('Total Changes')

plt.subplot(5, 1, 2)
plt.plot(ark_monthly['Start_Date'], ark_monthly['Cumulative_Changes'])
plt.title('Cumulative Changes')
plt.ylabel('Cumulative Changes')

plt.subplot(5, 1, 3)
plt.plot(ark_monthly['Start_Date'], ark_monthly['Minor_Versions'])
plt.title('Minor Versions per Month')
plt.ylabel('Minor Versions')

plt.subplot(5, 1, 4)
plt.plot(ark_monthly['Start_Date'], ark_monthly['Days_Since_Last_Update'])
plt.title('Days Since Last Update')
plt.ylabel('Days')

plt.subplot(5, 1, 5)
plt.plot(ark_monthly['Start_Date'], ark_monthly['Update_Frequency'])
plt.title('Update Frequency')
plt.ylabel('Updates per Day')

plt.tight_layout()
plt.show()

# Print summary statistics
print(ark_monthly.describe())

# Print the first few rows of the monthly data
print(ark_monthly.head())



# Define low and high activity based on percentiles
low_threshold = ark_monthly['Update_Frequency'].quantile(0.25)  # 25th percentile
high_threshold = ark_monthly['Update_Frequency'].quantile(0.75)  # 75th percentile

low_activity = ark_monthly[ark_monthly['Update_Frequency'] <= low_threshold].copy()
high_activity = ark_monthly[ark_monthly['Update_Frequency'] >= high_threshold].copy()

print("Periods of low update activity (bottom 25%):")
print(low_activity[['Start_Date', 'Total_Changes', 'Days_Since_Last_Update', 'Update_Frequency']])

print("\nPeriods of high update activity (top 25%):")
print(high_activity[['Start_Date', 'Total_Changes', 'Days_Since_Last_Update', 'Update_Frequency']])


# Visualize with new thresholds
plt.figure(figsize=(15, 7))
plt.plot(ark_monthly['Start_Date'], ark_monthly['Update_Frequency'], label='Update Frequency')
plt.title('Ark Update Frequency Over Time')
plt.xlabel('Date')
plt.ylabel('Update Frequency (Changes per Day)')
plt.axhline(y=low_threshold, color='r', linestyle='--', label='25th Percentile')
plt.axhline(y=high_threshold, color='g', linestyle='--', label='75th Percentile')
plt.legend()
plt.tight_layout()
plt.show()

# Calculate some statistics
print(f"\nMedian update frequency: {ark_monthly['Update_Frequency'].median():.2f} changes per day")
print(f"25th percentile (low activity threshold): {low_threshold:.2f} changes per day")
print(f"75th percentile (high activity threshold): {high_threshold:.2f} changes per day")

# Analyze patterns in low and high activity periods
low_activity.loc[:, 'Year'] = low_activity['Start_Date'].dt.year
high_activity.loc[:, 'Year'] = high_activity['Start_Date'].dt.year

print("\nLow activity periods by year:")
print(low_activity['Year'].value_counts().sort_index())

print("\nHigh activity periods by year:")
print(high_activity['Year'].value_counts().sort_index())

# Check for any seasonality
low_activity.loc[:, 'Month'] = low_activity['Start_Date'].dt.month
high_activity.loc[:, 'Month'] = high_activity['Start_Date'].dt.month

print("\nLow activity periods by month:")
print(low_activity['Month'].value_counts().sort_index())

print("\nHigh activity periods by month:")
print(high_activity['Month'].value_counts().sort_index())


from scipy.stats import ttest_ind

# Perform t-test between low and high activity periods
t_stat, p_val = ttest_ind(low_activity['Total_Changes'], high_activity['Total_Changes'])
print(f"T-Test results: t-statistic = {t_stat:.2f}, p-value = {p_val:.4f}")

# Correlation analysis
correlation_matrix = ark_monthly[['Total_Changes', 'Cumulative_Changes', 'Days_Since_Last_Update']].corr()
print("\nCorrelation Matrix:")
print(correlation_matrix)

# Visualize correlation matrix
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.title('Correlation Matrix of Monthly Features')
plt.show()


import pickle

# Define a dictionary to hold the analysis results
analysis_results = {
    'ark_update_data': ark_update_data,
    'ark_grouped_df': ark_grouped_df,
    'ark_monthly': ark_monthly,
    'low_activity': low_activity,
    'high_activity': high_activity,
    't_test_results': {
        't_stat': t_stat,
        'p_val': p_val
    },
    'correlation_matrix': correlation_matrix
}

# Save the analysis results to a pickle file
with open('ark_updates_analysis.pkl', 'wb') as file:
    pickle.dump(analysis_results, file)

print("Analysis results have been saved to ark_updates_analysis.pkl")
